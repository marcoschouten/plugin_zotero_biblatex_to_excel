@misc{dengFireFlowFast2024,
  title = {{{FireFlow}}: {{Fast Inversion}} of {{Rectified Flow}} for {{Image Semantic Editing}}},
  shorttitle = {{{FireFlow}}},
  author = {Deng, Yingying and He, Xiangyu and Mei, Changwang and Wang, Peisong and Tang, Fan},
  date = {2024-12-10},
  eprint = {2412.07517},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.07517},
  url = {http://arxiv.org/abs/2412.07517},
  urldate = {2025-03-03},
  abstract = {Though Rectified Flows (ReFlows) with distillation offers a promising way for fast sampling, its fast inversion transforms images back to structured noise for recovery and following editing remains unsolved. This paper introduces FireFlow, a simple yet effective zero-shot approach that inherits the startling capacity of ReFlow-based models (such as FLUX) in generation while extending its capabilities to accurate inversion and editing in \$8\$ steps. We first demonstrate that a carefully designed numerical solver is pivotal for ReFlow inversion, enabling accurate inversion and reconstruction with the precision of a second-order solver while maintaining the practical efficiency of a first-order Euler method. This solver achieves a \$3\textbackslash times\$ runtime speedup compared to state-of-the-art ReFlow inversion and editing techniques, while delivering smaller reconstruction errors and superior editing results in a training-free mode. The code is available at \$\textbackslash href\{https://github.com/HolmesShuan/FireFlow\}\{this URL\}\$.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcoschouten/Zotero/storage/7BCBS27C/Deng et al. - 2024 - FireFlow Fast Inversion of Rectified Flow for Image Semantic Editing.pdf;/Users/marcoschouten/Zotero/storage/UJXAIW5A/2412.html}
}

@inproceedings{kumariMultiConceptCustomization2023a,
  title = {Multi-{{Concept Customization}} of {{Text-to-Image Diffusion}}},
  booktitle = {2023 {{IEEECVF Conf}}. {{Comput}}. {{Vis}}. {{Pattern Recognit}}. {{CVPR}}},
  author = {Kumari, Nupur and Zhang, Bingliang and Zhang, Richard and Shechtman, Eli and Zhu, Jun-Yan},
  date = {2023-06},
  pages = {1931--1941},
  publisher = {IEEE},
  location = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.00192},
  url = {https://ieeexplore.ieee.org/document/10203856/},
  urldate = {2025-03-03},
  abstract = {While generative models produce high-quality images of concepts learned from a large-scale database, a user often wishes to synthesize instantiations of their own concepts (for example, their family, pets, or items). Can we teach a model to quickly acquire a new concept, given a few examples? Furthermore, can we compose multiple new concepts together? We propose Custom Diffusion, an efficient method for augmenting existing text-to-image models. We find that only optimizing a few parameters in the text-to-image conditioning mechanism is sufficiently powerful to represent new concepts while enabling fast tuning (âˆ¼ 6 minutes). Additionally, we can jointly train for multiple concepts or combine multiple fine-tuned models into one via closed-form constrained optimization. Our fine-tuned model generates variations of multiple new concepts and seamlessly composes them with existing concepts in novel settings. Our method outperforms or performs on par with several baselines and concurrent works in both qualitative and quantitative evaluations, while being memory and computationally efficient.},
  eventtitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9798350301298},
  langid = {english},
  file = {/Users/marcoschouten/Zotero/storage/TG475M6K/Kumari et al. - 2023 - Multi-Concept Customization of Text-to-Image Diffusion.pdf}
}

@misc{liuCompositionalVisual2023,
  title = {Compositional {{Visual Generation}} with {{Composable Diffusion Models}}},
  author = {Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B.},
  date = {2023-01-17},
  eprint = {2206.01714},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.01714},
  url = {http://arxiv.org/abs/2206.01714},
  urldate = {2025-03-03},
  abstract = {Large text-guided diffusion models, such as DALLE-2, are able to generate stunning photorealistic images given natural language descriptions. While such models are highly flexible, they struggle to understand the composition of certain concepts, such as confusing the attributes of different objects or relations between objects. In this paper, we propose an alternative structured approach for compositional generation using diffusion models. An image is generated by composing a set of diffusion models, with each of them modeling a certain component of the image. To do this, we interpret diffusion models as energy-based models in which the data distributions defined by the energy functions may be explicitly combined. The proposed method can generate scenes at test time that are substantially more complex than those seen in training, composing sentence descriptions, object relations, human facial attributes, and even generalizing to new combinations that are rarely seen in the real world. We further illustrate how our approach may be used to compose pre-trained text-guided diffusion models and generate photorealistic images containing all the details described in the input descriptions, including the binding of certain object attributes that have been shown difficult for DALLE-2. These results point to the effectiveness of the proposed method in promoting structured generalization for visual generation. Project page: https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/marcoschouten/Zotero/storage/443I8SS4/Liu et al. - 2023 - Compositional Visual Generation with Composable Diffusion Models.pdf;/Users/marcoschouten/Zotero/storage/EQDK376S/2206.html}
}

@misc{zhouFreeBlendAdvancing2025a,
  title = {{{FreeBlend}}: {{Advancing Concept Blending}} with {{Staged Feedback-Driven Interpolation Diffusion}}},
  shorttitle = {{{FreeBlend}}},
  author = {Zhou, Yufan and Shen, Haoyu and Wang, Huan},
  date = {2025-02-14},
  eprint = {2502.05606},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.05606},
  url = {http://arxiv.org/abs/2502.05606},
  urldate = {2025-03-03},
  abstract = {Concept blending is a promising yet underexplored area in generative models. While recent approaches, such as embedding mixing and latent modification based on structural sketches, have been proposed, they often suffer from incompatible semantic information and discrepancies in shape and appearance. In this work, we introduce FreeBlend, an effective, training-free framework designed to address these challenges. To mitigate cross-modal loss and enhance feature detail, we leverage transferred image embeddings as conditional inputs. The framework employs a stepwise increasing interpolation strategy between latents, progressively adjusting the blending ratio to seamlessly integrate auxiliary features. Additionally, we introduce a feedback-driven mechanism that updates the auxiliary latents in reverse order, facilitating global blending and preventing rigid or unnatural outputs. Extensive experiments demonstrate that our method significantly improves both the semantic coherence and visual quality of blended images, yielding compelling and coherent results.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/marcoschouten/Zotero/storage/WFHZKEW7/Zhou et al. - 2025 - FreeBlend Advancing Concept Blending with Staged Feedback-Driven Interpolation Diffusion.pdf;/Users/marcoschouten/Zotero/storage/B285KQ6D/2502.html}
}
